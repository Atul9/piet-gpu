// This is "kernel 2" (fill) in a 4-kernel pipeline. It processes the fill
// (polyline) items in the scene and generates a list of segments for each, for
// each tile.

#version 450
#extension GL_GOOGLE_include_directive : enable
#extension GL_KHR_shader_subgroup_ballot : enable
#extension GL_KHR_shader_subgroup_arithmetic : enable
#extension GL_KHR_shader_subgroup_shuffle : enable

layout(local_size_x = 32) in;

layout(set = 0, binding = 0) readonly buffer SceneBuf {
    uint[] scene;
};

layout(set = 0, binding = 1) buffer TilegroupBuf {
    uint[] tilegroup;
};

layout(set = 0, binding = 2) buffer FillSegBuf {
    uint[] fill_seg;
};

layout(set = 0, binding = 3) buffer AllocBuf {
    uint alloc;
};

#include "scene.h"
#include "tilegroup.h"
#include "fill_seg.h"

#include "setup.h"

// Factor into a separate .h file to avoid code duplication?

// We should be able to use gl_SubgroupSize, but that's problematic.
#define SUBGROUP_SIZE 32

shared PointRef sh_points[SUBGROUP_SIZE];
shared vec2 sh_start[SUBGROUP_SIZE];
shared vec2 sh_end[SUBGROUP_SIZE];

// Implementation of 32 x 32 boolean matrix transpose, using subgroups.
uint block_swap(uint a, uint m, uint s) {
    uint b = subgroupShuffleXor(a, s);
    uint c;
    if ((gl_SubgroupInvocationID & s) == 0) {
        c = b << s;
    } else {
        m = ~m;
        c = b >> s;
    }
    return (a & m) | (c & ~m);
}

const uint masks[5] = uint[5](0x55555555, 0x33333333, 0xf0f0f0f, 0xff00ff, 0xffff);

uint transpose(uint bitmask) {
    for (uint i = 0; i < 5; i++) {
        bitmask = block_swap(bitmask, masks[i], 1 << i);
    }
    return bitmask;
}

// Ensure that there is space to encode a segment.
void alloc_chunk(inout uint chunk_n_segs, inout FillSegChunkRef seg_chunk_ref,
    inout FillSegChunkRef first_seg_chunk, inout uint seg_limit)
{
    if (chunk_n_segs == 0) {
        if (seg_chunk_ref.offset + 40 > seg_limit) {
            seg_chunk_ref.offset = atomicAdd(alloc, SEG_CHUNK_ALLOC);
            seg_limit = seg_chunk_ref.offset + SEG_CHUNK_ALLOC - FillSegment_size;
        }
        first_seg_chunk = seg_chunk_ref;
    } else if (seg_chunk_ref.offset + FillSegChunk_size + FillSegment_size * chunk_n_segs > seg_limit) {
        uint new_chunk_ref = atomicAdd(alloc, SEG_CHUNK_ALLOC);
        seg_limit = new_chunk_ref + SEG_CHUNK_ALLOC - FillSegment_size;
        FillSegChunk_write(seg_chunk_ref, FillSegChunk(chunk_n_segs, FillSegChunkRef(new_chunk_ref)));
        seg_chunk_ref.offset = new_chunk_ref;
        chunk_n_segs = 0;
    }

}

void main() {
    uint tile_ix = gl_GlobalInvocationID.y * WIDTH_IN_TILES + gl_GlobalInvocationID.x;
    uint tilegroup_ix = gl_GlobalInvocationID.y * WIDTH_IN_TILEGROUPS
        + (gl_GlobalInvocationID.x / TILEGROUP_WIDTH_TILES);
    float blockx0 = float((gl_GlobalInvocationID.x / TILEGROUP_WIDTH_TILES) * TILEGROUP_WIDTH_PX);
    vec2 xy0 = vec2(gl_GlobalInvocationID.xy) * vec2(TILE_WIDTH_PX, TILE_HEIGHT_PX);
    TileGroupRef fill_start = TileGroupRef(tilegroup_ix * TILEGROUP_STRIDE + TILEGROUP_FILL_START);
    uint fill_n = tilegroup[fill_start.offset >> 2];

    FillTileHeaderRef tile_header_ref = FillTileHeaderRef(tile_ix * FillTileHeader_size);
    if (fill_n > 0) {
        ChunkRef chunk_ref = ChunkRef(fill_start.offset + 4);
        Chunk chunk = Chunk_read(chunk_ref);
        InstanceRef fill_ref = InstanceRef(chunk_ref.offset + Chunk_size);
        FillItemHeaderRef item_header = FillItemHeaderRef(atomicAdd(alloc, fill_n * FillItemHeader_size));
        FillTileHeader_write(tile_header_ref, FillTileHeader(fill_n, item_header));
        FillSegChunkRef seg_chunk_ref = FillSegChunkRef(0);
        FillSegChunkRef first_seg_chunk = FillSegChunkRef(0);
        uint seg_limit = 0;
 
        // Iterate through items; chunk.chunk_n holds count remaining.
        while (true) {
            if (chunk.chunk_n == 0) {
                chunk_ref = chunk.next;
                if (chunk_ref.offset == 0) {
                    break;
                }
                chunk = Chunk_read(chunk_ref);
                fill_ref = InstanceRef(chunk_ref.offset + Chunk_size);
            }
            uint n = 0;
            if (gl_SubgroupInvocationID < chunk.chunk_n) {
                Instance ins = Instance_read(Instance_index(fill_ref, gl_SubgroupInvocationID));
                PietFill fill = PietItem_Fill_read(PietItemRef(ins.item_ref));
                sh_points[gl_SubgroupInvocationID] = fill.points;
                n = fill.n_points - 1;
            }
            uint prefix_n = subgroupInclusiveAdd(n);
            uint sum_n = subgroupBroadcast(prefix_n, SUBGROUP_SIZE - 1);

            uint chunk_n_segs = 0;
            int backdrop = 0;
            uint ilast = 0;
            uint jlast = 0;
            for (uint k0 = 0; k0 < sum_n; k0 += SUBGROUP_SIZE) {
                uint my_ix = prefix_n - k0 - 1;
                uint is_last = subgroupOr(my_ix < SUBGROUP_SIZE ? 1 << my_ix : 0);
                // bit b of is_last is set when \exists x k0 + b + 1 == prefix_n[x]

                // Note: could use fancy subgroup ballot ops here (esp for sg > 32)
                // delta_i is subgroupBallotExclusiveBitCount(is_last)
                uint last_count = (is_last << (31 - gl_SubgroupInvocationID)) * 2;
                uint delta_i = bitCount(last_count);
                uint i_inv = ilast + delta_i;
                // else branch is gl_SubgroupInvocationID - subgroupBallotFindMSB(last_count & gl_SubgroupLtMask) - 1
                uint j_inv = delta_i == 0 ? jlast + gl_SubgroupInvocationID : 31 - findMSB(last_count);

                barrier();
                uint intersects = 0;
                if (i_inv < chunk.chunk_n) {
                    PointRef points = sh_points[i_inv];
                    vec2 start = Point_read(Point_index(points, j_inv)).xy;
                    vec2 end = Point_read(Point_index(points, j_inv + 1)).xy;
                    sh_start[gl_SubgroupInvocationID] = start;
                    sh_end[gl_SubgroupInvocationID] = end;
                    float y0 = max(xy0.y, min(start.y, end.y));
                    float y1 = min(xy0.y + float(TILE_HEIGHT_PX), max(start.y, end.y));
                    if (y0 < y1) {
                        float dy_inv = 1.0 / (end.y - start.y);
                        // Maybe fuzzy equality? There might also be a fancier way to do this
                        // with NaN behavior in min and max, but that sounds potentially flaky.
                        float u0 = start.y == end.y ? 0.0 : clamp((y0 - start.y) * dy_inv, 0.0, 1.0);
                        float u1 = start.y == end.y ? 1.0 : clamp((y1 - start.y) * dy_inv, 0.0, 1.0);
                        // maybe better to clamp after the mix (more precision)?
                        float x0 = mix(start.x, end.x, u0);
                        float x1 = mix(start.x, end.x, u1);
                        float xmin = max(min(x0, x1) - blockx0, 0.0);
                        float xmax = clamp(max(x0, x1) - blockx0, 0.0, float(TILEGROUP_WIDTH_PX));

                        uint x0i = uint(floor(xmin * (1.0 / TILE_WIDTH_PX)));
                        uint x1i = uint(ceil(xmax * (1.0 / TILE_WIDTH_PX)));
                        intersects = x1i > x0i ? (2 << (x1i - 1)) - (1 << x0i) : 0;
                    }
                }

                intersects = transpose(intersects);
                barrier();

                uint bitmask = intersects | is_last;
                while (bitmask != 0) {
                    uint k = findLSB(bitmask);
                    if ((intersects & (1 << k)) != 0) {
                        alloc_chunk(chunk_n_segs, seg_chunk_ref, first_seg_chunk, seg_limit);
                        vec2 start = sh_start[k];
                        vec2 end = sh_end[k];
                        FillSegment seg = FillSegment(start, end);
                        FillSegment_write(FillSegmentRef(seg_chunk_ref.offset + FillSegChunk_size + FillSegment_size * chunk_n_segs), seg);
                        chunk_n_segs++;
                    }

                    if ((is_last & (1 << k)) != 0) {
                        FillItemHeader_write(item_header, FillItemHeader(backdrop, first_seg_chunk));
                        if (chunk_n_segs != 0) {
                            FillSegChunk_write(seg_chunk_ref, FillSegChunk(chunk_n_segs, FillSegChunkRef(0)));
                            seg_chunk_ref.offset += FillSegChunk_size + FillSegment_size * chunk_n_segs;
                            chunk_n_segs = 0;
                        }
                        item_header.offset += FillItemHeader_size;
                        backdrop = 0;
                    }

                    bitmask &= bitmask - 1; // clear bottom bit
                }

                ilast += bitCount(is_last);
                jlast = is_last >= (1 << 31) ? 0 : subgroupBroadcast(j_inv, SUBGROUP_SIZE - 1) + 1;
            }

            uint this_n = min(chunk.chunk_n, SUBGROUP_SIZE);
            fill_ref.offset += this_n * Instance_size;
            chunk.chunk_n -= this_n;
        }
    } else {
        // As an optimization, we could just write 0 for the size.
        FillTileHeader_write(tile_header_ref, FillTileHeader(fill_n, FillItemHeaderRef(0)));
    }
}
